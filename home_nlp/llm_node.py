"""
llm_node.py

A ROS 2 node that provides a conversational Large Language Model (LLM) service.

This node subscribes to transcribed speech from the "transcription" topic
(`std_msgs/msg/String`) and processes it through a Large Language Model
pipeline built on Hugging Face and LangChain. The node maintains a chat
history per session, allowing for multi-turn, context-aware dialogue.

The model is loaded from Hugging Face Hub using a token specified via the
`HF_TOKEN` environment variable, and is initialized during configuration
for reduced response latency. Prompt templates support system instructions,
few-shot examples, and conversation history.

Responses generated by the LLM are published to the "llm_response" topic
as `std_msgs/msg/String`.

Dependencies:
    - transformers==4.54.1
    - langchain==0.3.27
    - langchain-huggingface==0.3.1
    - python-dotenv==1.1.1
    - torch==2.7.1
    - accelerate==1.9.0
    - jinja2==3.1.6

Author: Enfu Liao, Chinlu Chen
Date: 2025-08-01
"""

import os
from typing import List

import rclpy
import torch
import queue
from dotenv import load_dotenv
from langchain.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from rclpy.lifecycle import LifecycleNode, LifecycleState, TransitionCallbackReturn
from std_msgs.msg import String
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from home_nlp.prompt_template import example_prompt, examples, system_prompt

torch.set_float32_matmul_precision("high")


class LargeLanguageModelNode(LifecycleNode):
    def __init__(self):
        super().__init__("llm_node")

        self.declare_parameter("model", "google/gemma-3-1b-it")
        # Qwen/Qwen2.5-1.5B-Instruct
        # meta-llama/Llama-3.2-1B-Instruct
        # meta-llama/Llama-3.1-8B-Instruct
        # google/gemma-3-1b-it
        # google/gemma-3-4b-it
        # deepseek-ai/deepseek-coder-6.7b-instruct
        # microsoft/Phi-4-mini-instruct
        # mistralai/Mistral-7B-Instruct-v0.3
        # meta-llama/Llama-3.1-8B-Instruct

        self.declare_parameter("period", 1.0)

        self.transcription_buffer = queue.Queue()

        self.pub = None
        self.sub = None
        self.chat_with_history = None
        self.chat_map = {}

    def on_configure(self, state: LifecycleState) -> TransitionCallbackReturn:
        self.get_logger().info("Configuring LLM Node...")

        try:
            self.model_name = self.get_parameter("model").value
            self.get_logger().info(f"Loading model: {self.model_name}")

            load_dotenv()
            hf_token = os.getenv("HF_TOKEN")
            if hf_token is None:
                self.get_logger().error("HF_TOKEN not found in environment.")
                return TransitionCallbackReturn.FAILURE

            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=hf_token,
            )
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map="auto",
                torch_dtype="auto",
                token=hf_token,
            )

            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.95,
                repetition_penalty=1.1,
                return_full_text=False,
            )

            llm = HuggingFacePipeline(pipeline=pipe)
            chat_model = ChatHuggingFace(llm=llm, model_id=self.model_name)

            few_shot_prompt = FewShotChatMessagePromptTemplate(
                example_prompt=example_prompt,
                examples=examples,
            )

            prompt_template = ChatPromptTemplate.from_messages(
                [
                    ("system", system_prompt),
                    few_shot_prompt,
                    MessagesPlaceholder(variable_name="history"),
                    ("user", "{user_input}"),
                ]
            )

            chain = prompt_template | chat_model | StrOutputParser()
            self.chat_with_history = RunnableWithMessageHistory(
                chain,
                get_session_history=self.get_chat_history,
                input_messages_key="user_input",
                history_messages_key="history",
            )

            self.pub = self.create_publisher(String, "llm_response", 10)
            _ = self.create_timer(
                self.get_parameter("period").value,
                self.timer_cb,
            )

            self.get_logger().info("Configured")
            return TransitionCallbackReturn.SUCCESS

        except Exception as e:
            self.get_logger().error(f"Configuration failed: {e}")
            return TransitionCallbackReturn.FAILURE

    def on_activate(self, state: LifecycleState) -> TransitionCallbackReturn:
        self.get_logger().info("Activating LLM Node...")

        try:
            self.sub = self.create_subscription(
                String, "transcription", self.transcription_cb, 10
            )
            self.get_logger().info("Activated")
            return TransitionCallbackReturn.SUCCESS
        except Exception as e:
            self.get_logger().error(f"Activation failed: {e}")
            return TransitionCallbackReturn.FAILURE

    def on_deactivate(self, state: LifecycleState) -> TransitionCallbackReturn:
        self.get_logger().info("Deactivating LLM Node...")

        if self.sub:
            self.destroy_subscription(self.sub)
            self.sub = None
            self.get_logger().info("Subscription destroyed.")

        self.chat_map.clear()
        return TransitionCallbackReturn.SUCCESS

    def on_cleanup(self, state: LifecycleState) -> TransitionCallbackReturn:
        self.get_logger().info("Cleaning up LLM Node...")
        self.chat_with_history = None
        return TransitionCallbackReturn.SUCCESS

    def on_shutdown(self, state: LifecycleState) -> TransitionCallbackReturn:
        self.get_logger().info("Shutting down LLM Node...")
        return TransitionCallbackReturn.SUCCESS

    def get_chat_history(self, session_id: str) -> InMemoryChatMessageHistory:
        if session_id not in self.chat_map:
            self.chat_map[session_id] = InMemoryChatMessageHistory()
        return self.chat_map[session_id]

    def transcription_cb(self, msg: String):
        self.get_logger().debug("Received transcription")
        try:
            self.transcription_buffer.put_nowait(msg.data)
        except queue.Full:
            self.get_logger().warn("Transcription buffer full, dropping chunk.")

    def timer_cb(self):
        if self.transcription_buffer.empty():
            return

        user_input = self.transcription_buffer.get_nowait()

        response = self.chat_with_history.invoke(
            {"user_input": user_input}, config={"session_id": "default"}
        )
        self.get_logger().debug(f"User input: {user_input}\nLLM output:\n{response}")

        response_msg = String()
        response_msg.data = response
        if self.pub:
            self.pub.publish(response_msg)


def main(args: List[str] | None = None):
    rclpy.init(args=args)
    node = LargeLanguageModelNode()
    try:
        node.trigger_configure()
        node.trigger_activate()
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == "__main__":
    main()
